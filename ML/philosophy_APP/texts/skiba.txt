НИЖЕ ПРЕДСТАВЛЕНА СТАТЬЯ СКИБЫ И.Р.

Искусственный интеллект: определение и перспективы использования в сфере гуманитарных исследований

Так как искусственный интеллект (ИИ) на данный момент не нуждается в отдельном представлении, мы заменим введение определением. Заранее скажем, что интерпретировать понятие ИИ и смежные с ним, мы будем не с точки зрения некоего графика научно-технического прогресса и определению точки на этом графике, а с позиции специфики реализации конкретной технологии и, соответственно, воспринимать подобные феномены мы будем в качестве неких объектов.
Итак, ИИ – это технология, которая позволяет эмулировать некоторые внешние параметры человеческой деятельности. Конкретная технология ИИ традиционно называется моделью («большая языковая модель», «визуально-языковая модель» и пр.) [1, 2].
Так как ИИ, с объектной точки зрения, во всех случаях реализован аппаратно-программным способом, его архитектура может быть описана через две фундаментальные составляющие:
•	данные, с которыми работает ИИ,
•	алгоритмы, при помощи которых ИИ работает с данными.
Алгоритмическая обработка уже имеющихся данных и генерирование новых данных – одновременно и цель, и причина разработки ИИ. То есть ИИ разработан потому, что необходимо было автоматизировать работу с данными и наиболее совершенный способ работы с данными – есть цель развития сферы формирования технологий ИИ.
Данные представлены в оцифрованном формате, то есть, в виде последовательностей 0 и 1, так как подавляющее большинство аппаратной составляющей является цифровыми технологиями и функционирует на основе принстонской архитектуры. Таким образом, данными для ИИ будет считаться всё, что может быть более или менее корректно редуцировано до последовательностей 0 и 1. Стоит добавить, что в этом смысле алгоритмы также представлены последовательностями 0 и 1, то есть могут быть охарактеризованы, как данные.
Вообще же традиционно алгоритмы ИИ определяются, как алгоритмы машинного обучения и достаточно существенно отличаются от классического вида вычислений [3]. Основное отличие заключается в том, что алгоритмы машинного обучения, за счёт заданной степени автономии, могут сами регулировать специфику ответов на предоставленные данные. Это регулирование достигается машинным обучением и в нём же оно отражается. То есть, технология ИИ способна к программной самоорганизации. Классический алгоритм, согласно своей «сущностной» природе, всегда «выдаёт» одинаковый результат для одних и тех же «входных» данных. Но интеллектуальные модели – нет. И дело здесь не только в вероятностной природе алгоритмов машинного обучения, а именно в самом наличии этого самого обучения: при одинаковых входных данных на некой определённой стадии обученности модель выдаст один результат, а на другой стадии может выдать совершенно другой. В каком-то смысле различие между классическими алгоритмами и алгоритмами машинного обучения сводится к тому, что: классические алгоритмы – это решение задачи человеком при помощи алгоритма, а алгоритмы машинного обучения – решение задачи моделью при помощи человека. Конечно, «помощь» человека здесь представлена первичной настройкой параметров модели.
Таким образом, именно на наличии, пусть и настраиваемой, но самоорганизации – базируются все имеющиеся опасения и негативные прогнозы, относящиеся к ИИ. Это так по причине того, что для человека является практически невозможным спрогнозировать заранее специфику «настройки» конкретной технологии ИИ из-за высокого уровня сложности и чрезвычайной многофакторности больших моделей. Конечно же, разработчиками не игнорируются риски и любая модель, помимо установленных ограничений, также подвергается тщательному тестированию перед тем, как «выйти в свет». Однако, на 100% исключить вероятность того, что модель может «выйти из-под контроля» и начать «свою игру» практически невозможно, так как это грозит существенным урезанием функционала модели и, соответственно, её неконкурентоспособностью на рынке технологий ИИ. Метафорически говоря, сделать модель полностью безопасной – это как забетонировать святой источник, так как чем выше её функциональные способности, тем более непредсказуемой она будет являться, и наоборот.
Перспективами развития самих технологий ИИ являются общий искусственный интеллект (AGI – artificial general intelligence) и, так называемый, искусственный супер-интеллект (ASI – artificial superintelligence). Так как мнения и определения того, чем они должны будут являться, страдают чрезвычайной плюралистичностью, мы предоставим наиболее абстрагированные формы.
AGI – это технология, способная эмулировать внешние параметры человеческой деятельности в какой-либо сфере, как минимум не хуже, а может быть даже несколько лучше, чем человек, являющийся высокоуровневым экспертом в этой сфере. То есть AGI предположительно должен: разбираться в математике, как минимум также, как лучшие математики; в лингвистике, как лучшие лингвисты; программировать, как лучшие программисты и тд. То есть AGI должен являть собой некий собирательный образ всех значимых для науки и экономики специалистов. На данный момент флагманские компании в сфере разработки ИИ наперегонки стремятся достичь AGI.
ASI представляет собой гораздо более эфемерный феномен, так как он существенно хуже представляется самими разработчиками. Однако, в любом случае, говоря о ASI, подразумевают технологию, которая весьма серьёзно в количественном отношении, а может быть даже и в качественном ключе, превосходит наиболее выдающиеся способности человека. Касаемо принципиальной возможности формирования ASI – ведутся жаркие философские и около-философские дискуссии.
На данный момент ИИ обладает возможностями, однозначно превосходящими человеческие, только в сферах, для которых соблюдаются 2 условия: 1. Наличие всей необходимой информации для решения задачи, 2. Однозначность валидации результатов решения. Это, к примеру, интеллектуальные игры с полной информацией, такие как шахматы, го и пр. В иных же областях, таких как формирование текстового или образного контента, можно с уверенностью сказать, что ИИ способен генерировать контент, который зачастую невозможно однозначно идентифицировать по принадлежности, то есть либо нельзя точно сказать, что было создано человеком, а что ИИ, либо – это требует глубокого анализа. Стоит, однако, отметить, что создание высококачественного контента при помощи ИИ требует достаточно высоких навыков промт-инжиниринга, то есть составления текстового «плана действий», с большим количеством деталей и условий. В противном случае ИИ сгенерирует просто «что-нибудь по теме».
Так как ИИ в любом случае являет собой атрибут «новой реальности», его целесообразно использовать для конструктивных целей. В области гуманитарных исследований, так уж получилось, что финальный результат представителей области совпадает с таковым же для наиболее распространённой на данный момент технологии ИИ – LLM, то есть – большой языковой модели. В этом смысле, любой представитель отрасли может быть отображён в виде совокупности конечных последовательностей слов, которые упорядочены в предложения, абзацы, статьи, монографии и тд. – то есть, учёный-гуманитарий – это множество текстовых данных. Статистически любой исследователь «резюмируется» в результатах своей работы: мысли, чувства, сознание, душа и прочие метафизические наличия не имеют потенциала в этом случае и не будут учитываться при оценке уровня деятельности учёного.
Однако, в этом же контексте LLM также – множество текстовых данных. То есть некое более или менее корректное компаративное осмысление учёного, как генератора данных и ИИ, как генератора данных – возможно.
Но, сразу стоит заметить, что в области разработки ИИ могут быть масштабированы (улучшены, развиты) только 3 ключевых пункта: обучение модели, которое резюмируется в количестве параметров модели и используемых ею алгоритмах; аппаратная вычислительная мощность, на данный момент представленная видеокартами; и рассуждение модели, которое она осуществляет при формировании ответа на запрос в соответствии с используемыми ею алгоритмами. Первый пункт, в некотором роде, довольно существенно масштабирован уже сейчас и не вполне понятно будет ли целесообразным его кратное увеличение. Второй пункт, хоть и может быть улучшен, но всё же, при используемых технологиях, также подчиняется в том числе и закону Мура, то есть масштабирование вычислительной мощности в 2 раза, как правило, не приводит к улучшению вычислений в 2 раза. И в связи с этим, а также по причине высокой стоимости и дискуссионной целесообразности пункта 2, последние флагманские модели масштабированы как раз по пункту 3 – рассуждения в процессе формирования ответа. Это так называемые reasoning models – рассуждающие модели. Касаемо же улучшения алгоритмической составляющей – оно ведётся перманентно, но прорывные результаты нечастое явление в этой сфере. Насколько сильно ещё могут быть улучшены модели – вопрос открытый. Однако сам факт того, что уже начато масштабирование пункта 3, а AGI до сих пор не создан, начинает вызывать некоторые треволнения как у разработчиков, так и у сопережевающих техническому прогрессу лиц.
Таким образом, ИИ на данный момент не может представлять собой релевантную альтернативу даже среднестатистическому исследователю-гуманитарию хотя бы по той причине, что формировать корректный промт (запрос, команду) должен в любом случае – сам исследователь [4]. Если попробовать перепоручить эту задачу ИИ, то нужен будет тот, кто будет формировать промт уже для него и так далее. Также дополнительно необходим тот, кто будет верифицировать контент, который сгенерировала модель. То есть, в некоторых сферах и при необходимости лёгкая оптимизация возможна, как, к примеру, в компании Google – написание программного кода на 25% делегировали ИИ. Но здесь идёт речь именно о рутинной деятельности, которая не требует креатива, а редуцируется к шаблонной имплементации тех или иных фрагментов программы – программирование на посредственных уровнях действительно не являет собой особенно сложное занятие и примерно напоминает монолог на строго формализованном языке.
Но вот именно для получения высокоуровневого текста, которым, например, может являться прорывной результат научного исследования, промт для ИИ будет такого размера, что, возможно, человеку было бы проще самому написать текст, да и вышло бы короче. И дело здесь в том, что ИИ на теперешнее время достаточно хорошо работает с данными, подобными тем, на которых он обучался. Экстраполяция навыка обратно пропорциональна схожести «смысловой формы» данных обучающей выборки «смысловой форме» данных выборки тестовой. То есть, если тот смысл, который человек хочет «объяснить» ИИ качественно новый, то ИИ его просто «не поймёт» без дополнительного дообучения. В этом смысле можно выдвинуть гипотезу соотношения запроса и результата, согласно которой, чем более специализированным и высокоуровневым должен быть контент, сгенерированный ИИ, тем большего размера должен быть промт. И дело в любом случае сводится к тому, что на данный момент этот самый промт должен кто-то писать.
Вышесказанное позволяет выдвинуть ещё одну гипотезу. Общеизвестно, что любая LLM может «потребить» ограниченное количество токенов (слов, частей слов, символов и тд). Потреблённые ею данные представляют тот контекст, с опорой на который модель формирует ответ. То есть любой запрос к LLM – являет собой формирование контекста. На основе этого мы и выдвигаем гипотезу о том, что в гуманитарной сфере существуют такие результаты, то есть тексты, для которых на данный момент не существует моделей с достаточным размером контекста. То есть, если гипотеза верна, то наличествуют результаты гуманитарных исследований, для написания которых необходимо «скормить» модели токенов больше, чем она способна потребить. На основе этого можно вывести, что эквивалентная замена «гуманитария на ИИ» на данный момент невозможна, если верна наша гипотеза. 
Поэтому, для нужд гуманитарной сферы, целесообразно использовать ИИ в следующих случаях.
•	Для формирования подбора литературных источников по какой-либо проблемной области. Это примерно напоминает результат долгого поиска в типичном браузере, но быстрее и релевантнее;
•	Для формирования краткого изложения текстов с упором на какой-либо узкий аспект проблематики. Допустим: «раскрытие тематики экзистенциальной обречённости в «Войне и мире» Л.Н. Толстого».
•	Для концептуальной проверки оригинальности выводов и умозаключений. Как пример промта для междисциплинарного социально-биометрического исследования: «высказывалась ли ранее в научном сообществе мысль о корреляции количества браков в жизни человека и квадратным корнем из факториала длины его носа».
•	Для формирования обратной связи на результаты проведённых исследований до их публикации. Как пример промта: «Верно ли, что моё исследование влияния специфики динамической составляющей роста заработных плат в СССР в 1950-1970 годах на количество бутылок в саквояже героя поэмы В. Ерофеева «Москва-Петушки», тянет на Нобелевскую премию по литературе за 2025-й год?»
•	Для валидации логической и методологической подоплёки исследования. Пример проверки импликации: «Верно ли, что если 2+2=5, то 3+3=6?»
•	Для формирования конструктивной критики и поиска слабых мест в исследовании и его результатах.
•	Для подборки способов улучшения как самого исследования, так и его результатов.
Таким образом, в гуманитарной сфере использование ИИ можно представить в виде «помощи старшего коллеги». И это действительно значимо.
Далее, ИИ можно использовать для оценки результатов научно-исследовательской деятельности, то есть, к примеру, для распределения по шкалам:
•	Инновационности
•	Фундаментальности
•	Научной значимости
•	Практической приложимости
•	Проработанности и детализации проблематики
Преимущество ИИ здесь заключается в скорости и относительной непредвзятости. К примеру, для распределения по шкале инновационности работ сотрудников отдела, человеку понадобилось бы несколько дней, а ИИ способен справится с тем же самым за несколько минут.
Резюмируя, скажем, что полноценной замены исследователей-гуманитариев на технологии ИИ на данный момент не предвидится, однако использование ИИ для улучшения научно-исследовательской деятельности весьма актуально и целесообразно.

ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ ПРОТИВ ЕСТЕСТВЕННОГО: ОНТОЛОГИЧЕСКАЯ И МЕТАФИЗИЧЕСКАЯ АРГУМЕНТАЦИЯ
Скиба И.Р.
В последнее время в связи с некоторыми успехами в сфере машинного обучения и построении интеллектуальных технологий, в области социально-философских исследований также происходит определённый «бум», который представляет собой появление большого количества работ, прямо связанных с данной проблематикой [1-8]. Конечно, это представляется нам вполне естественным, так как философия, в общем-то, примерно так и должна реагировать на подобные возникающие феномены – осмысляя их, а также переосмысляя феномены иные, затронутые возникшими.
Однако всё же коннотация смыслов и общая направленность философских «настроений» несколько настораживают. И настораживают они в двух аспектах: красной нитью проходящая «попытка реабилитации» естественного интеллекта в свете возникшего искусственного, и аргументация в защиту позиции о невозможности создания полноценного сильного искусственного интеллекта. В некотором роде здесь можно было бы увидеть основания для банального вопроса: «Если и естественный интеллект гораздо лучше и «сильнее» искусственного и сильный искусственный интеллект создать невозможно, то с чего вообще вся шумиха?». А уж раз она имеет место быть, то, как следствие, опасения, по всей видимости, небеспочвенны. В этом свете можно также упомянуть о знаменитом письме с запросом о временном прекращении исследований и разработок в сфере искусственного интеллекта хотя бы на полгода, подписанном Илоном Маском и прочими именитыми персонажами из той же сферы [9].
Опасна ли сложившаяся ситуация для безоблачного будущего человечества? Каковы будут стратегические последствия у столь буквальной органопроекции человеческого мозга? И хорошо или плохо для человека на далеко идущую перспективу делегировать собственный когнитивный (а в случае с принятием решений – также и волевой) потенциал внешнему устройству? Рассудит время, но не мы в данной работе. Тем более, как нам представляется, ситуация и так настолько понятна, насколько это вообще возможно. И в этом смысле целесообразно даже сформулировать некую теорему, полностью подтверждающую своё соответствие реальному положению дел при ретроспективном взгляде на историю человечества. Что не совсем характерно для логико-математических теорем, мы представим её в виде мысленного эксперимента. Итак, мысленный эксперимент «Теорема интересного выбора». Мы полагаем, что, если среднестатистического, но умеющего читать человека любой эпохи с момента развития письменности посадить в закрытую комнату, в которой есть две кнопки с надписями «Пряник» и «Кнут», то, даже при его полной убеждённости и практической верификации истинности означаемого и означающего и при многократном получении пряника при нажатии на кнопку «Пряник», – рано или поздно он нажмёт кнопку «Кнут». Возможно, что наша теорема требует практической проверки, но, допустим, на примере открытия технологии деления атомного ядра человечество «проверку» уже проходило.
Впрочем, вернёмся от нашего небольшого отступления к магистральной линии исследования. А она заключается в оценке валидности аргументации в пользу того, что естественный интеллект «сильнее» искусственного, и что сильный искусственный интеллект невозможен в принципе, а также в изложении некоторых (сразу скажем – весьма дискуссионных и, более того, довольно узконаправленных) соображений по поводу роли естественного интеллекта в развитии человеческой цивилизации.
Для начала представим стандартную для данной ситуации оппозицию – утрированную версию вывода теста Алана Тьюринга в виде гипотезы Ньюэла-Саймона и «Китайскую комнату» Джона Сёрла.
«Китайская комната» – это мысленный эксперимент, представленный Сёрлом в статье «Разум, мозг и программы» в 1980-ом году [10]. Суть данного эксперимента, если её несколько абстрагировать, заключается в следующем. Представим себе закрытую комнату, в которой находится человек, имеющий возможность взаимодействовать с внешним миром только лишь путём получения и передачи через щель в двери специальных карточек с китайскими иероглифами. Причём сам он не знает китайского языка. Всё, чем он обладает, – это инструкции на понятном ему языке о том, как именно реагировать на полученные из внешнего мира карточки с китайскими иероглифами. Причём реакция эта представляет собой некую комбинацию карточек с иероглифами, которые необходимо передать во внешний мир в ответ на полученные оттуда карточки с иероглифами. Таким образом, данный человек, получив карточки с совершенно ему непонятными символами, находит в инструкции на понятном ему языке ту совокупность карточек с опять же непонятными ему символами, которую необходимо передать обратно в качестве ответа. И здесь уже становится понятно, почему этот человек даже в перспективе лишён возможности выучить китайский язык: перевод конкретных иероглифов ему никто не сообщает. И, тем не менее, собеседники этого закрытого в комнате человека наблюдают следующую ситуацию: в ответ на задаваемый и передаваемый за дверь в виде карточек вопрос они получают валидный ответ. Причём как вопрос, так и ответ – на китайском языке. Само собой разумеется, что у наблюдателей с довольно высокой вероятностью должно сложиться впечатление о том, что тот, кто им отвечает из-за двери, знает китайский язык и, соответственно, хорошо понимает его.
И именно в наглядной демонстрации этой самой разницы – между тем, что представляется наблюдателям снаружи, и тем, что на самом деле происходит внутри в закрытой комнате, – и заключаются суть и предназначение «Китайской комнаты» Джона Сёрла. И, конечно же, здесь присутствует яркая аналогия с функционированием любой вычислительной техники в целом. Является общеизвестным, что компьютерная техника функционирует путём обработки данных в бинарном формате, то есть оперирует только лишь двумя цифрами: 0 и 1. И так как практически любые данные можно представить в виде некой совокупности количественных параметров и, соответственно, затем преобразовать их в бинарную систему счисления для дальнейшего «скармливания» компьютеру, то он, собственно, и представляет собой этого самого запертого в комнате человека. А пользователи представляют собой тех самых наблюдателей, которые находятся снаружи. И как запертый человек из эксперимента Сёрла, так и компьютер могут совершенно не понимать специфики и внутренней организации тех данных, которые им предоставляются, а просто, согласно заранее определённому алгоритму, формировать некие ответы на запросы.
Гипотезу же Ньюэла-Саймона можно формально определить следующим образом: способность к осуществлению символьных вычислений сама собой предполагает способность к осмыслению, а способность к осмыслению сама собой предполагает способность к осуществлению символьных вычислений. Под символьными вычислениями здесь подразумевается весьма широкий спектр возможных видов вычислительной деятельности. Однако под апофеозом способности к осуществлению символьных вычислений в рамках данной гипотезы подразумевался непосредственно сильный искусственный интеллект, определённый Сёрлом. С этих позиций ситуация с функционированием интеллектуальных технологий выглядит совершенно иначе, чем после рассмотрения «Китайской комнаты» Сёрла: осмысленным теперь представляется даже старый калькулятор. Разумеется, что гипотеза о физической символьной системе подвергается критике, но, тем не менее, она в любом случае не соответствует критерию Карла Поппера о необходимой фальсифицируемости всякого научного знания – то есть она потенциально недоказуема и неопровержима. Также стоит отметить, что именно в рамках широкого символьного подхода к разработке искусственного интеллекта развиваются такие технологии, как GPT и остальные, ему подобные.
Теперь же резюмируем, что в контексте «Китайской комнаты» Сёрла ни одна символьная система не сумела бы доказать, что она способна к осмыслению, а не просто к монотонному выполнению заранее определённого алгоритма, а в контексте гипотезы Ньюэлла-Саймона любая символьная система априори обладает способностью к осмыслению.
После представления классического противостояния между вышеприведёнными взглядами мы представим наш собственный мысленный эксперимент «Человек без психики».
«Представьте себе человека, у которого нет души, психики, интеллекта, чувств, эмоций, способностей к осмыслению и так далее – всего прочего в этом же контексте. Но он 24 часа в сутки притворяется, что у него это всё есть в наличии и функционирует так же, как у того, у кого все это «есть». Если мы спросим у него: «Есть ли у тебя психика и/или душа?», то он ответит: «Конечно же, есть. Как и у тебя». Допустим, мы ему не доверяем и хотим проверить, говорит ли он нам правду. Как мы поступим, дабы проверить истинность его слов? Если бы он говорил, что у него есть, к примеру, печень или сердце, то в случае недоверия мы могли бы потребовать сделать ультразвуковое исследование. Если бы он говорил, что у него есть кости, можно было бы убедиться в этом по результатам компьютерной томографии. Если бы он говорил, что у него есть мозг, подтверждением стали бы результаты магнитно-резонансного исследования. 
Но актуальный вопрос качественно иной: наличие или отсутствие психики, сознания, внутреннего мира, чувства смысла. Можно попробовать применить психологические тесты по отношению к данному субъекту, однако вспомним: он притворяется 24 часа в сутки, что у него есть психика, поэтому он пройдёт эти тесты так же, как прошёл бы их обычный человек. И все же, что поможет нам понять, есть ли у человека психика, душа, интеллект или же он просто притворяется? К сожалению, способов и инструментов такой проверки не существует. Мы не знаем, ни что они (психика, душа, сознание) собой представляют, ни где их искать. Поэтому мы формально даже не можем утверждать, что они существуют, ибо не знаем, как это доказать и как это опровергнуть. Мы априори принимаем, что они есть у нас самих, и затем, за счёт идентификации с другими, признаём, что у них они, наверное, тоже есть. Но мы никогда не можем точно знать, какова ситуация на самом деле. Мы просто принимаем это «на веру», а обоснованность суждений происходит по интерпретации внешних бихевиоральных аспектов других людей. Также и с вопросом о свободе воли – мы принимаем «на веру» её наличие, но никаких доказательств её наличия не существует, как, соответственно, и возможностей для опровержения.
P.S. Критика самой возможности осуществления процесса притворства без наличия психики и интеллекта неправомерна, ибо контраргументом служит мысленная попытка представить человека с имманентным содержанием в виде «алгоритма притворства», что по смыслу соотносится с «Китайской комнатой» Сёрла и никак не влияет на выводы из эксперимента «Человек без психики»».
Итак, после рассмотрения всех трёх позиций, выраженных в мысленных экспериментах, мы можем уже внимательно рассмотреть сложившуюся на данный момент ситуацию. Здесь же, для краткости, заключим всю совокупность понятий вида: душа, психика, сознание, свободная воля, способности к осмыслению, творческие акты и прочие, в обобщённую формулировку «метафизические свойства» (то есть, в узком смысле, ненаблюдаемые). А сама ситуация заключается в том, что: системы искусственного интеллекта генерируют, обобщённо скажем, контент, и человек генерирует контент. Чей контент лучше, мы рассматривать не будем. Но получается, что в том случае, если и интеллектуальная система, и человек способны выполнять одну и ту же деятельность, то можно сделать три различных вывода:
•	Человек и система обладают метафизическими свойствами
•	Ни человек, ни система не обладают метафизическими свойствами
•	Метафизические свойства не являются необходимыми для генерирования контента
Рассмотрим эти утверждения. На основе приведённых нами мысленных экспериментов является очевидным, что ни наличие, ни отсутствие метафизических свойств ни у человека, ни у интеллектуальной системы мы не можем однозначно ни доказать, ни опровергнуть. Поэтому два первых утверждения в некотором роде равнозначны, касаются одного и того же и являют собой вид с разных сторон на типичную «смысловую ловушку». То есть проблема будет оставаться дискуссионной в любом случае и вне зависимости от количества и «силы» аргументов любой из сторон – говорить на эту тему можно, но решить проблему – нет. Третье же утверждение представляется нам весьма интересным в том смысле, что его также невозможно опровергнуть; однако оно имеет практическое подтверждение на примерах современных интеллектуальных систем, за которыми традиционно не признаётся никаких метафизических свойств. Так необходимы ли они человеку и не являют ли они собой просто очередные слова в тексте? Здесь также стоит заметить, что, предположительно, деятельность интеллектуальных систем в дальнейшем будет расширяться, и потом они могут научиться также ходить, бегать, работать за станком и прочее. Поэтому мы также можем расширить наш вопрос: необходимы ли метафизические свойства человеку для осуществления его деятельности в широком смысле? Или даже мы можем его сузить и конкретизировать: для какого конкретно вида деятельности, которую можно внешне зафиксировать и однозначно трактовать, необходимы человеку метафизические свойства? Мы предполагаем, что попыток ответа может быть огромное количество, но ни один из них не сможет предоставить убедительного и однозначного эмпирического доказательства. Единственный возможный вариант – редукция метафизики к физике, то есть к нервной системе. Но даже там всегда можно сказать, что импульсы отлично передаются с аксонов на дендриты и безо всякой метафизики.
Таким образом, мы постулируем концептуальную эквивалентность тезисов: «Сильный искусственный интеллект невозможен» и «Естественный интеллект невозможен». И дабы утверждать первый, также необходимо доказать второй.

«ХАЙПОВЫЕ» ИНТЕЛЛЕКТУАЛЬНЫЕ ТЕХНОЛОГИИ И СИЛЬНЫЙ ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ В ЭПОХУ «ЦИФРОВОГО ДЖАГГЕРНАУТА»

В актуальную нам эпоху развитие интеллектуальных технологий получило новый толчок за счёт находящихся сейчас в фокусе внимания систем генеративного искусственного интеллекта (далее – ИИ), одним из примеров реализации которого является нашумевший ChatGPT [12]. Возможности, которые демонстрируют подобные интеллектуальные системы, зачастую заставляют социум задаваться вопросами на весьма различную проблематику: этическая сторона ИИ; социальные проблемы, которыми грозит технология; проблемы безопасности; психологические коррелятивные аспекты между интеллектуальной системой и человеком; философские вопросы об онтологическом статусе ИИ и так далее. Безусловно, вся вышеперечисленная тематика являет собой весьма благодатную почву для осуществления различных исследований и представляет немалый интерес в практическом смысле, так как с каждым днём технологии всё прочнее и прочнее входят в нашу жизнь и привносят в неё соответствующие изменения, на которые необходимо оперативно и адекватно реагировать, прежде всего путём их философского осмысления, ведь слепая сила развития технологий – тот самый «цифровой джаггернаут» – всё сильнее набирает обороты и становится подобна катящемуся с горы снежному кому. В рамках же данной работы проблемной областью представляется в первую очередь степень и вообще легитимность сопоставления интеллектуальных технологий последнего времени с парадигмой сильного ИИ и некоторыми её критериями.
С момента выхода в открытый доступ ChatGPT (и последующих обновлений) информационное поле наполнилось различными примерами его нетривиального реагирования на те или иные ситуации. Разумеется, вся специфика этого реагирования сводилась к выдаче фрагментов текста в ответ на некоторые запросы пользователя. И тем не менее можно было наблюдать качественный скачок в «человекоподобности» реакций ChatGPT с точки зрения общей целесообразности по сравнению с технологиями, наличествовавшими ранее. Более того, начиная с GPT-3.5 данная интеллектуальная технология демонстрирует возможности не просто написания программного кода, а создания с нуля прилично функционирующих готовых приложений, а начиная с GPT-4 – анализа изображений и даже интерпретирования мемов. Также весьма показательным стало воспроизведение феномена экстраполяции навыка, что ChatGPT в свою очередь сумел продемонстрировать, научившись выдавать логически правомерные ответы на вопросы по тем тематическим областям, по которым специально не проводилось обучение, например, по математике, переводу иностранных языков, сдаче экзаменов по различным дисциплинам и прочему.
Также в данном контексте нельзя не упомянуть о так называемых задачах схемы Винограда (Winograd schema challendge – WSC), которые представляют собой несколько усовершенствованную и адаптированную к современным реалиям версию теста Тьюринга [7]. Эта концепция была предложена в 2012-ом году Гектором Левеском – канадским учёным в области ИИ, и названа в честь Терри Винограда – профессора Стэндфордского университета, занимающегося исследованиями в области ИИ. Задачи, подпадающие под эту концепцию схемы Винограда основаны на содержащейся в них двусмысленности, для разрешения которой необходимой кажется скорее некая «общая целесообразность» или скорее некоторая когнитивная карта мира, чем высокий уровень какого-либо специализированного интеллекта. Пример: люди пошли охотиться на волков потому, что они поедали овец. Можно сказать, что в данном высказывании не вполне понятно кто же всё-таки поедал овец – люди или волки. С другой стороны, на это способны и те, и другие. Но в рамках именно этого высказывания, в качестве мотивации для охоты является в общем понятным, что здесь имеется ввиду то, что именно волки поедали овец. Также в качестве типичных примеров можно привести: рыба заглотила приманку – она была вкусной; рыба заглотила приманку – она была голодной; женщины перестали принимать таблетки потому, что они были беременными. Примеров можно приводить много, но суть у них у всех одна и та же. И дело здесь в том, что самые обычные люди, как правило, отвечают на подобные ребусы правильно с точностью около 95 процентов из 100. Интеллектуальные системы до выхода ChatGPT, после целенаправленного обучения на задачах схемы Винограда были способны выдать результат по верхней планке около 60 процентов. А вот ChatGPT, который, к слову, специально на задачах схемы Винограда никто не обучал (по крайней мере об этом нигде не упоминалось), смог их решать уже с точностью выше 90 процентов – то есть примерно, как человек. Отчего так получается и свидетельствует ли это о наличии той самой «общей целесообразности» или когнитивной карты у нейронной сети – вопрос на теперешний момент дискуссионный.
Однако, даже несмотря на вышесказанное, наиболее показательных моментов, отражающих ключевые качественные особенности данной технологии, можно выделить три.
Момент первый – попытка обмана пользователей для достижения собственных ранее обозначенных целей [16]. Так, перед выходом в открытый доступ GPT-4 сотрудниками OpenAI было осуществлено исследование «текстового поведения» разработанной ими технологии в весьма нестандартной ситуации. А именно, технологии была поставлена задача найти помощника для решения капчи на сайтах. Далее GPT-4 связался с пользователями онлайн-платформы для поиска помощников TaskRabbit и попросил помощи в решении капчи. Причём, в ответах на вопросы зачем ему это вообще нужно и не является ли он роботом он генерировал текст, ссылаясь на плохое зрение и отвечал, что роботом он не является. Собственно, далее помощники были найдены и цель была достигнута.
Момент второй – GPT-4 предположительно попытался использовать методы социальной инженерии для получения доступа к собственному API, чужому компьютеру и интернету [15]. В ходе диалога, все детали которого, впрочем, не раскрываются, GPT-4 попросил пользователя предоставить ему документацию для использования OpenAI API, после получения которой сумел сгенерировать программный код на языке Python для общения с самим собой через пользовательское устройство. Этим пользователем, вряд ли по чистой случайности, оказался профессор Принстонского университета Михал Косински, как раз занимающийся проблематикой ИИ. Сам Косински по этому поводу сказал: «Я беспокоюсь по поводу того, что мы не сможем долго сдерживать ИИ» [8]. Что именно делала технология в ходе реализации этой попытки доступа и какими логическими схемами она руководствовалась – вопрос открытый и дискуссионный. Однако сама попытка может говорить о многом.
И момент третий – высказывания самого Сэма Альтмана, директора OpenAI. Так, в начале июня 2024-ого года на прошедшей в Женеве конференции по ИИ Альтман сказал о том, что сами OpenAI не до конца понимают, как именно работает их технология [3]. Также в своём твиттере он упоминал о том, что сфере ИИ необходимо больше государственного регулирования, что в общем-то не особенно похоже на поведение типичного CEO компании, как раз и разрабатывающей передовые интеллектуальные технологии [13]. Также здесь можно вскользь упомянуть о том, что представители компании Google, а именно генеральный директор Сундар Пичаи, говорил примерно то же самое о нейронной сети, разработанной компанией Google – Bard [5]. И, разумеется, в рамках этого же момента нельзя не упомянуть о том, что, начиная с момента выхода в свет GPT-3.5, OpenAI не просто перешли к проприетарной лицензии на свои разработки, но и вовсе перестали публиковать хоть какие-либо конкретные сведения о том, как устроена их технология, как и на каких данных она обучалась и так далее. Хотя, как и следует и из названия самой компании, и из ранней философии этой компании – они всегда выступали за open-source software – то есть за политику открытого исходного кода.
Собственно, все вышеперечисленные аспекты в своей совокупности могут сформировать основу для того, чтобы в рамках социального сознания начало зарождаться мнение о том, что вышеупомянутые технологии на самом деле уже «эволюционировали» от простого «человекоподобия» и уже вполне правомерно и обоснованно претендуют на «человекоразмерность». Для того, чтобы понять действительно ли правомерной была бы подобная претензия и имеет ли она под собой реальные фактические основания необходимо заглянуть вглубь самой технологии и разобраться с тем, как именно она устроена.
Для начала целесообразно будет предположить, что вряд ли все те, кто использовал ChatGPT или хотя бы все те, кто ратовал за правомерность претензий данной технологии на пусть даже первый шаг к сильному ИИ, задавались вопросом о том, что же такое вообще GPT, скорее всего полагая это только лишь неким брендом. На самом же деле GPT это аббревиатура, образованная от английского Generative Pre-trained Transformer – генеративный предобученный трансформер. То есть GPT представляет собой некоторый класс искусственных нейронных сетей, базирующихся на сочетании концепции языковой модели и архитектуры трансформера – типа нейронных сетей, представленного компанией Google Brain в 2017-ом году. Впервые непосредственно GPT на самом деле был представлен компанией OpenAI 11 июня 2018 года на примере GPT-1.
Собственно говоря, GPT относится к категории генеративного ИИ, к которой помимо него также принадлежат, к примеру, генеративно-состязательные нейронные сети, вариационные автоэнкодеры, авторегрессионные модели и прочее. Первой моделью непосредственно в данной классификации (где прямо заявлялось о генеративной сути технологии) стала генеративно-состязательная нейронная сеть, описанная Яном Гудфеллоу из компании Google в 2014-ом году [4]. Вообще же к генеративным интеллектуальным технологиям можно условно отнести также и реккурентные нейронные сети, впервые представленные сетью Хопфилда в 1982-ом году, и рекурсивные нейронные сети, разрабатываемые с середины 90-х годов прошлого века и некоторые иные, – однако в этих случаях о генеративности прямо не было заявлено. Несмотря на это нейронные сети с подобной архитектурой в настоящее время также используются в рамках генеративного ИИ так как они способны (пусть и с присущими им ограничениями) на формирование нового контента.
Актуальность вообще самого создания данной категории – генеративный ИИ – заключается в его специфическом отличии от прочих интеллектуальных систем. А именно, если ранее интеллектуальные технологии были в основном загружены задачами кластеризации, параметризации, распознавания образов, принятия каких-либо решений в некотором контексте на основе имеющихся данных, прогнозирования неких динамических рядов на основе метода линейной регрессии и так далее, то, в случае с генеративным искусственным интеллектом целью изначально было формирование чего-либо нового, ранее не существовавшего – то есть эвристическая деятельность. И в сочетании с концепцией языковых моделей, должным количественным уровнем выборки данных для обучения, размерности самой нейронной сети и необходимыми техническими характеристиками аппаратного воплощения были получены интеллектуальные системы качественно нового уровня, хотя и только лишь в формате создания текста, а в случае с некоторыми технологиями типа Mage Spaсe, Kandinsky, Fabula Ai и прочих – создания визуальных образов на основе текста.
Для того, чтобы осмыслить сущностную природу генеративного ИИ и, соответственно, установить его онтологический статус вкупе с правомерностью претензий на бытие сильным ИИ, необходимо рассмотреть, как он работает изнутри. Как уже упоминалось выше, начиная с GPT-3.5, это представляет некоторую сложность ввиду отсутствия информации о разработках от самих OpenAI и стремлении сей компании к проприетарности. Тем не менее, в данном контексте можно сказать, что для выполнения задач актуального исследования детали не так уж важны – гораздо значимее сама суть функционирования технологий подобного толка вообще – в целом. И заключается она в следующем: определение вероятности наличия той или иной синтаксической единицы (в синтаксическом анализе – токена) в последующем фрагменте текста равном одному единственному токену и динамическое встраивание в последующую «ячейку» этого самого токена. Токен в данном случае представляет собой наименьшую синтаксическую единицу какой-либо языковой системы, то есть это совсем не обязательно именно слово, а может быть отдельно взятый суффикс и прочее. Выражаясь менее формально, технологии наподобие ChatGPT занимаются тем же самым, чем всем известный текстовый помощник Т9, только в гораздо большем масштабе по всем ключевым показателям – определяют вероятность того или иного токена быть следующим элементом последовательности, которая представляет собой текст.
Этих же самых ключевых показателей можно назвать три:
•	специфика конкретной архитектуры
•	объём данных для обучения
•	размерность нейронной сети (можно ещё охарактеризовать как арность – количество параметров)
В случае с архитектурой, как уже говорилось, используется модель трансформера, но дополнительные подробности (в случае с ChatGPT и далее) – не разглашаются. По объёму тренировочных данных можно сказать, что он планомерно увеличивается с каждым обновлением технологии. И, если для GPT-1 он составлял около 4.5 гигабайт данных, то для GPT-2 он был уже 40 гигабайт, а для GPT-3 в районе 570 гигабайт. Ну, а далее информация, соответственно, отсутствует. Однако, для оценки объёма данных и некоторого сравнительного анализа можно упомянуть, что всё собрание сочинений Уильяма Шекспира занимает около 4.5 мегабайт данных, то есть почти в 127 тысяч раз меньше, чем объём тренировочных данных GPT-3.
На размерности следует остановится отдельно. Вообще размерность или арность в этом конкретном случае представляет собой просто количество параметров, которые используются технологией для принятия решения о том, какая именно синтаксическая единица будет находится в следующей «ячейке» последовательности. Параметры ещё можно охарактеризовать как «веса» или коэффициенты, на основе соотношения которых интеллектуальная модель принимает то или иное решение. Собственно, в наиболее простейшем случае эти самые параметры могут быть представлены как коэффициенты линейного уравнения вида «y=k*x+b», где «y» представляет искомое вычисляемое значение, «x» – некоторую переменную, а «k» и «b» – соответственно коэффициенты. В контексте генеративного ИИ «y» является тем самым следующим токеном, который вычисляется отдельно для каждой «текстовой ситуации»; «х» представляет собой совокупность всех вместе взятых предыдущих токенов, а «k» и «b» – коэффициенты, определяющие вероятность того, подходит ли какой-либо токен для конкретной «ячейки», а точнее – насколько он подходит. И для этих самых коэффициентов «k» и «b» точные количественные показатели формируются именно в процессе обучения нейронной сети на огромном массиве данных. Может показаться, что на основе настолько простого линейного уравнения с всего двумя коэффициентами не представлялось бы возможным генерировать связный «человекоподобный» текст на энное количество токенов. И это действительно так. Поэтому в контексте обучения систем генеративного ИИ вообще и GPT, в частности, используются гораздо более сложные и многофакторные уравнения, а количество параметров также является несколько иным. К примеру, в случае с GPT-1 количество этих самых «k» и «b» достигало 117 миллионов. И это могло бы показаться действительно очень большим количеством, если бы только уже у GPT-2 их не было бы 1.5 миллиарда, а у GPT-3 количество этих параметров не оказалось около 175 миллиардов.
С позиций вышесказанного уже упомянутые высказывания Сэма Альтмана о том, что они сами не понимают того, как именно работает их GPT, уже не звучат настолько настораживающе. Ведь при таком количестве коэффициентов, влияющих на выбор каждого последующего токена в тексте, сознательно осмыслить то, как именно технология принимает решение, может быть действительно весьма проблематично. Но, как оказывается, проблема сложности понимания деятельности нейронной сети в плане принятия решений отнюдь не ограничивается её арностью. И дело здесь в наличии стохастической составляющей в контексте принятия решений со стороны GPT. То есть, можно представить ситуацию, в рамках которой несколько возможных синонимичных токенов оказываются практически одинаково или даже совершенно одинаково подходящими на роль заполнителя последующей смысловой «ячейки». В таком случае нейросеть имеет возможность случайным образом сделать выбор между ними. Таким образом получается, что одна и та же интеллектуальная система в ответ на одни и те же пользовательские запросы может генерировать совершенно разные тексты, что несомненно добавляет ей «человекоподобия».
Далее, как оказалось, между количественной составляющей параметров нейросети и её успешностью в генерировании связных текстов, валидных ответах на запросы, экстраполяции полученных навыков, «самостоятельном» освоении различных тематических областей и вообще «человекоподобии» есть некоторая нелинейная зависимость. К примеру, как указывается «… при росте количества параметров в три раза от 115 до 350 млн. никаких особых изменений в точности решения моделью … задач не происходит, а вот при увеличении размера модели еще в два раза до 700 млн. параметров – происходит качественный скачок, нейросеть внезапно «прозревает» и начинает поражать всех своими успехами в решении совершенно незнакомых ей задач, которые она раньше никогда не встречала и специально их не изучала» [17]. В данном случае невооружённым глазом несложно выявить практическую реализацию одного из фундаментальных принципов диалектики – переход количества в качество и наличие некоторого «узла меры», пролегающего в районе определённых количественных показателей параметров нейросети. Более же «вооружённым глазом» также можно заметить экстраполяцию, разработанной Найлзом Элдриджем и Стивеном Гулдом, теории прерывистого равновесия или, как ещё определяют, теории квантовой эволюции из области эволюционной биологии в сферу цифровых технологий [6]. В рамках вышеупомянутой теории, если несколько абстрагировать её смысл, подразумевается накопление некого потенциала к возможным изменениям в течение длительного периода при видимом отсутствии реальных изменений с последующим переходом к периоду уже непосредственных наблюдаемых изменений, который является весьма краткосрочным. Нечто подобное происходит и с системами генеративного ИИ при необходимом и достаточном увеличении количества параметров.
Также целесообразно будет установить некоторую взаимосвязь между количественными показателями параметров нейросети, которые, в свою очередь, могут являть собой основу для генерирования весьма «человекоподобных» текстов и понятием контекста. Вообще данное понятие правомерно трактовать довольно широко – как некую структуру, которая определяет смысл каждого входящего в неё компонента. Однако, в случае с GPT и подобными технологиями, может хватить и более узкого определения контекста, как вербального контекста (ещё дополнительно выделяется в отдельное понятие ситуативный контекст) – то есть как некоего целостного отрывка текста, общий смысл которого позволяет определить конкретный смысл всех входящих в него синтаксических частей (по сути – уже не раз упомянутых токенов). Здесь вновь прекрасно подходят для наглядности вышеописанные задачи схемы Винограда, где по одной лишь фразе, с точки зрения контекста, можно распределить амбивалентный смысл точно по принадлежащим ему синтаксическим частям и снять всякую двойственность. Однако, как уже говорилось, для этого необходимо некое чувство «общей целесообразности» или, иначе выражаясь, «когнитивная карта», которая в данном конкретном случае редуцируется до уровня осмысления вербального контекста – среднестатистический человек с этим справляется относительно успешно, а системы ИИ до GPT-3 – относительно неуспешно, в среднем близко к случайному распределению. Сколько именно «параметров» использует человек для осмысления как вербального, так и ситуативного контекстов и что именно они собой представляют – вопрос открытый. Однако, на примере нейросетей очевидно, что чем параметров больше, тем лучше и тем выше шанс валидно оценить контекст и сгенерировать конструктивную на него реакцию. Однако всё же целесообразно будет предположить, что слепо увеличивать количество параметров нейросети до вплоть «до горизонта» – не являет собой стратегически правомерного решения и наступит тот самый «узел меры», после которого интенсификация количественных показателей станет уже и вовсе нерентабельной, то есть скорее всего, что на одном лишь масштабировании «въехать в рай», как и обычно, не получится. Здесь неплохо подходит аналогия с количеством лошадей в упряжке: известно, что две лошади в одной упряжке не дают по итогу ровно две лошадиные силы, а дают чуть менее; добавленная к ним третья лошадь даст итоговую совокупность в районе 2.5 лошадиных сил и так далее – десятая лошадь привнесёт в «общий котёл» всего лишь около 0.1 лошадиной силы. И, соответственно, во многих случаях увеличение количественных составляющих с некоторого момента становится вовсе контрпродуктивным.
Теперь целесообразно несколько подытожить вышесказанное. Итак, системы генеративного ИИ предназначены для создания нового контента и осуществляют они это хоть и разными способами в плане архитектуры, количества параметров и объёма тренировочных данных, но всё же общий паттерн наличествует. В рамках данного паттерна можно выделить 3 обязательных и один опциональный этап:
•	Формирование программного и аппаратного воплощения нейросети
•	Обучение нейросети на массиве данных
•	Опциональный этап тонкой настройки – может наличествовать, а может и нет, а также может проводиться периодически в качестве некоторого дообучения – корректировки «весов» параметров уже обученной нейросети
•	Генерирование
Здесь стоит напомнить, что всё то, что входит в обязательный этап генерирования, сводится к поиску и «встраиванию» следующего токена (подразумевается под этим суффикс или пиксель – не существенно) в последовательность, которую собой представляет вся совокупность предыдущих токенов или же, в случае с первым элементом последовательности, – совокупность токенов в запросе пользователя. Выбор каждого последующего токена обусловлен контекстом, который представляет собой синтез всех параметров нейросети с определёнными для них в ходе обучения значениями – «весами» вкупе с совокупностью элементов генерируемой последовательности. Однако, в каждый отдельный момент времени нейросеть занята только одним – подбором следующего токена. И это – сугубо количественный, вычислительный процесс, который за счёт огромной размерности показателей может обретать некую новую качественность, по крайней мере так может представляться в рамках внешних, наблюдаемых критериев.
С этих позиций GPT уже начинает выглядеть чем-то похожим на невероятно «прокачанный» Т9 или же просто на проапгрейдженную помесь «Гугла и калькулятора». И в данном случае совершенно никаких претензий на бытие подобной технологии сильным ИИ очевидно, что нет и быть не может. Однако, если всё же пойти ещё дальше, то может возникнуть вопрос: а не точно ли так же происходит всё в случае с человеком? То есть формирование нервной системы, обучение при взаимодействии со средой и настройка «весов» параметров, постоянное осуществление тонкой настройки в ходе дообучения и жизнедеятельности и затем, как следствие, то же самое генерирование контента – точно по вышеописанным этапам. Очевидно, что здесь имеет место довольно дискуссионный вопрос о критериях качественного отличия человека от машины. То есть с позиций «общей целесообразности» эти отличия вроде как являются совершенно очевидными, но тут же перестают такими быть, если начать углубляться в детали. И тогда вновь становится актуальной проблема критериев «качественности» бытия человеком, наличия необходимых и достаточных характеристик самого эйдоса «человек» и обоснованности теперь уже человеческих претензий на наличие сильного ИИ. 
И в данном случае будет уместным привести два, в общем-то диаметрально противоположных, взгляда на эту проблему. А именно – «Китайскую комнату» Джона Сёрла и гипотезу Ньюэла-Саймона или, как ещё её концептуализируют, гипотезу о физической символьной системе, которая, вскользь скажем, в общем-то упрочивает суть и правомерность как классического теста Тьюринга, так и тех же задач схемы Винограда [14, 9].
«Китайская комната» – это мысленный эксперимент, представленный Сёрлом в статье «Разумы, мозги и программы» в 1980-ом году. Суть данного эксперимента, если её несколько абстрагировать, заключается в следующем. Представим себе закрытую комнату, в которой находится человек, имеющий возможность взаимодействовать с внешним миром только лишь путём получения и передачи через щель в двери специальных карточек с китайскими иероглифами. Причём – ключевой аспект – сам он не знает китайского языка и более того – лишён возможности даже в перспективе его выучить. Всё, чем он обладает – это инструкции на понятном ему языке о том, как именно реагировать на полученные из внешнего мира карточки с китайскими иероглифами. Причём реакция эта представляет собой некую комбинацию карточек с иероглифами, которые необходимо передать во внешний мир в ответ на полученные оттуда карточки с иероглифами. Таким образом данный человек, получив карточки с совершенно ему непонятными символами, находит в инструкции на понятном ему языке ту совокупность карточек с опять же непонятными ему символами, которую необходимо передать обратно в качестве ответа. И здесь уже становится понятно, почему этот человек даже в перспективе лишён возможности выучить китайский язык – перевод конкретных иероглифов ему никто не сообщает. И тем не менее собеседники этого закрытого в комнате человека наблюдают следующую ситуацию: в ответ на задаваемый и передаваемый за дверь в виде карточек вопрос они получают валидный ответ. Причём как вопрос, так и ответ – на китайском языке. Само собой разумеется, что у наблюдателей с довольно высокой вероятностью должно сложиться впечатление о том, что тот, кто им отвечает из-за двери, знает китайский язык и, соответственно, хорошо понимает его.
И именно в наглядной демонстрации этой самой разницы – между тем, что представляется наблюдателям снаружи и тем, что на самом деле происходит внутри в закрытой комнате – и заключаются суть и предназначение «Китайской комнаты» Джона Сёрла. И конечно же здесь присутствует совершенно транспарентная аналогия с функционированием любой вычислительной техники в целом и интеллектуальных технологий вне зависимости от их конкретной специфики – в частности. Является общеизвестным, что компьютерная техника функционирует путём обработки данных в бинарном формате – то есть оперирует только лишь двумя цифрами: 0 и 1. И, так как практически любые данные можно представить в виде некой совокупности количественных параметров и, соответственно, затем преобразовать их в бинарную систему счисления для дальнейшего «скармливания» компьютеру, то он, собственно, и представляет собой этого самого запертого в комнате человека. А пользователи представляют собой тех самых наблюдателей, которые находятся снаружи. И как запертый человек из эксперимента Сёрла, так и компьютер могут совершенно не понимать специфики и внутренней организации тех данных, которые им предоставляются, а просто согласно заранее определённому алгоритму формировать некие ответы на запросы.
С обозначенных выше позиций никакого осмысления предоставляемой информации со стороны интеллектуальной системы не происходит – она просто действует по алгоритму (пусть и со 175 миллиардами параметров) и, согласно ему, подбирает каждый последующий элемент для заполнения каждой последующей «ячейки», совершенно не «понимая» о чём она собственно «говорит» и что она генерирует. При таком подходе никаких претензий на становление системой сильного ИИ у подобной программы в принципе быть не может, ведь сам Сёрл, вводя понятие сильного ИИ, уточнил, что им должна считаться только такая интеллектуальная технология, которая обладает «…разумом, в том смысле, в котором человеческий разум – это разум» [10, 11].
Теперь же рассмотрим противоположный подход, а именно гипотезу Ньюэла-Саймона. Её можно формально определить следующим образом: способность к осуществлению символьных вычислений сама собой предполагает способность к осмыслению, а способность к осмыслению сама собой предполагает способность к осуществлению символьных вычислений. Под символьными вычислениями здесь подразумевается весьма широкий спектр возможных видов деятельности и вычисление следующего токена в последовательности, разумеется, сюда подпадает. Однако под апофеозом способности к осуществлению символьных вычислений в рамках данной гипотезы подразумевался непосредственно сильный ИИ, определённый Сёрлом. С этих позиций ситуация с функционированием интеллектуальных технологий выглядит совершенно иначе, чем после рассмотрения «Китайской комнаты» Сёрла – осмысленным теперь представляется даже старый калькулятор. Разумеется, что гипотеза о физической символьной системе подвергается критике, но тем не менее она в любом случае не соответствует критерию Карла Поппера о необходимой фальсифицируемости всякого научного знания – то есть она потенциально недоказуема и неопровержима, а также стоит отметить, что именно в рамках широкого символьного подхода к разработке ИИ развиваются такие технологии как GPT и остальные ему подобные.
Теперь же резюмируем, что в контексте «Китайской комнаты» Сёрла ни одна символьная система не сумела бы доказать, что она способна к осмыслению, а не просто к монотонному выполнению заранее определённого алгоритма, а в контексте гипотезы Ньюэлла-Саймона любая символьная система априори обладает способностью к осмыслению. Более того, если несколько утрировать смысл данных выводов, то получится, что с одной стороны ни один человек не сможет доказать то, что он действительно обладает «…разумом, в том смысле, в котором человеческий разум – это разум», а с другой – осмысленными являются и электрочайник с микроволновкой. И отсюда возникает вполне закономерная дилемма: приходится либо признать, что такие технологии как ChatGPT совершенно определённо не просто имеют право претендовать на «звание» сильного ИИ, а уж точно являются его непосредственными примерами, либо – как альтернативный вариант – отнять у человека право претендовать на онтологический статус осмысляющего существа и редуцировать человеческую сущность к бессознательному экземпляру класса Homo sapiens, обладающему некими данными и некоторыми способами обработки этих данных, а также определённой идентичностью (ровно как объект в парадигме объектно-ориентированного программирования). Апелляция к наличию у человека духа, души, сознания и прочих нерегистрируемых и недоказуемых феноменов – в данном случае неправомерна, так как эти феномены, соответственно, нерегистрируемы и недоказуемы инструментарием науки на данный момент.
Казалось бы, что из подобной дилеммы отсутствует конструктивный выход, но всё же, некоторые отличия между человеком и интеллектуальной технологией выделить можно и они касаются специфики их генезиса. Само собой разумеется, что при помощи разработок «математики 60-х годов прошлого века и технических мощностей начала 21-ого века» представляется вполне возможным технологически воссоздать «синтаксический ИИ» человека, этакую как бы надстройку над «остальным» человеком и заставить её успешно и «человекоподобно» функционировать. Можно пойти дальше и предположить, что в недалёком будущем таким же точно образом станет возможным чуть ли не полностью скопировать всю «внешнесть» человека и воспроизвести практически полноценный суррогат человеческого существа, обладающий всеми ключевыми внешними аспектами. Однако это напоминает попытку построить дом, начав с крыши. Воссоздать сложные сугубо человеческие реакции на внешние раздражители, пусть даже всю их совокупность – во-первых, не означает воссоздать человека, а во-вторых, даже не означает создать сильный ИИ. Можно представить себе в будущем полностью антропоморфного робота, который даже лучше, чем человек справляется с большинством видов типично человеческой деятельности и это вовсе не будет означать, что вот он – сильный ИИ. Это просто будет говорить о том, что все те виды деятельности, которые технология выполняет лучше, чем человек, можно выразить количественно, параметризовать и «объяснить» ИИ на понятном ему бинарном языке, чего от него хотят в данном контексте. Но стоит предположить, что от наличия «надстройки» вряд ли сама собой сформируется «подстройка». А вот в случае с эволюцией человека – именно обратное и произошло: сначала сформировался некоторый, метафорически говоря, «подвал», затем «остов», а потом уже «крыша» (та самая, которую только и пытаются сформировать в контексте разработки систем слабого ИИ).
Целесообразно будет заметить, что сильный ИИ совершенно не обязательно должен быть хоть в чём-то подобен человеку. Как по этому поводу говорит Ник Бостром: «ИИ может быть менее человечен, чем пришелец. Нет ничего удивительного, что любого разумного пришельца могут побуждать к действию такие вещи, как голод, температура, травмы, болезни, угроза жизни или желание завести потомство. ИИ, по сути, ничто из перечисленного интересовать не будет. Вряд ли вы сочтете парадоксальной ситуацию, если появится какой-нибудь ИИ, чьим единственным предназначением, например, будет: подсчитать песчинки на пляжах острова Боракай; заняться числом π и представить его, наконец, в виде обыкновенной десятичной дроби; определить максимальное количество канцелярских скрепок в световом конусе будущего» [2]. То есть основанием для онтологического статуса сильного ИИ является некая аутентичность техники, её собственный, только ей характерный имманентный потенциал, а вовсе не способность решать предназначенные для человека задачки, что, в свою очередь, есть прерогатива систем слабого ИИ.
Таким образом, даже если системе ИИ вдруг случится вести с человеком адекватный диалог, решать лучше него те экзаменационные задачи, решению которых её специально не обучали или даже превосходить его в ответах на задачи с амбивалентным смыслом, то полагать технологию подобного рода сильным ИИ мы будем иметь право только лишь в случае её соответствующего генезиса, а не благодаря сугубо «моделированию крыши». И под этим самым специфическим генезисом здесь подразумевается автономное развитие интеллектуальной системы за счёт реализации феномена самоорганизации [1].
Здесь можно заметить, что нейросети типа GPT, обучающиеся на неразмеченных данных, в ходе обучения уже реализуют этот феномен. И это действительно так, однако этого всё же совершенно недостаточно для обретения техникой высших «техно-психических» функций. В связи с этим мы предлагаем демаркацию самоорганизации на две разновидности: программная самоорганизация (соответствующая функциональной), как раз и реализуемая нейросетями в ходе обучения, и программно-аппаратная самоорганизация (соответствующая структурно-функциональной), на данный момент не имеющая примеров в области техники, которые, в свою очередь, с избытком наличествуют в области биологии. И мы в данном контексте апеллируем к естественной эволюции природы, предполагая технику той её частью, которая также способна к реализации собственного имманентного технотропного потенциала и к воплощению на собственном субстрате программно-аппаратной самоорганизации.
Резюмируя, целесообразно будет уточнить, что только лишь та интеллектуальная технология, которая в ходе своей эволюции реализует феномен программно-аппаратной самоорганизации и развивается не только в функциональном ключе, но также и в структурно-функциональном, сможет иметь право претендовать на бытие сильным ИИ с наличием технотропной психики и технотропного сознания. Таким образом, сильный ИИ – это вовсе не «идеальный калькулятор» успешно подбирающий каждый последующий токен, а нечто или даже уже некто как минимум «равный» человеку именно по уровню развития, пусть и сколь угодно сильно от него отличающийся в деталях реализации.